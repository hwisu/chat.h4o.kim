import { ChatMessage, OpenRouterModel, OpenRouterModelsResponse } from '../types';

// ìš”ì•½ ì„¤ì •
export interface SummarizationConfig {
  maxTokensBeforeSummary: number;  // 24000í† í° (128k ì»¨í…ìŠ¤íŠ¸ì˜ ì•½ 20%)
  summaryTargetTokens: number;     // 800í† í° (ì§§ê²Œ)
  targetRetainTokens: number;      // 10000í† í° (ìµœê·¼ ëŒ€í™” ìœ ì§€ ëª©í‘œ)
  minRetainTokens: number;         // 6000í† í° (ìµœì†Œ ìœ ì§€)
  maxRetainTokens: number;         // 15000í† í° (ìµœëŒ€ ìœ ì§€)
}

export const DEFAULT_SUMMARY_CONFIG: SummarizationConfig = {
  maxTokensBeforeSummary: 24000,
  summaryTargetTokens: 800,
  targetRetainTokens: 10000,
  minRetainTokens: 6000,
  maxRetainTokens: 15000
};

// Summary prompt
const SUMMARY_PROMPT = `Please provide a concise summary of the following conversation, focusing only on the key points. Make sure to include all important information:

Conversation:
{conversation}

Summary (key points only):`;

// ìš”ì•½ìš© ëª¨ë¸ ìºì‹œ (5ë¶„)
let summaryModelsCache: { models: OpenRouterModel[], timestamp: number } | null = null;
const SUMMARY_CACHE_DURATION = 5 * 60 * 1000; // 5ë¶„

// ìš”ì•½ìš© ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°
function getSummaryModelPriority(modelId: string): number {
  const id = modelId.toLowerCase();

  // 1ìˆœìœ„: google, flash, freeê°€ ëª¨ë‘ ë“¤ì–´ê°„ ëª¨ë¸
  if (id.includes('google') && id.includes('flash') && id.includes('free')) {
    return 1;
  }

  // 2ìˆœìœ„: gemma 3 freeê°€ ë“¤ì–´ê°„ ëª¨ë¸
  if (id.includes('gemma') && id.includes('3') && id.includes('free')) {
    return 2;
  }

  // 3ìˆœìœ„: ê¸°íƒ€ google free ëª¨ë¸
  if (id.includes('google') && id.includes('free')) {
    return 3;
  }

  // 4ìˆœìœ„: ê¸°íƒ€ free ëª¨ë¸
  if (id.includes('free')) {
    return 4;
  }

  return 5; // ë‚®ì€ ìš°ì„ ìˆœìœ„
}

// ìš”ì•½ìš© ëª¨ë¸ í•„í„°ë§ ë° ì •ë ¬
function filterSummaryModels(models: OpenRouterModel[]): OpenRouterModel[] {
  // free ëª¨ë¸ë§Œ í•„í„°ë§
  let summaryModels = models.filter(m => m.id.endsWith(':free'));

  // ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬
  summaryModels = summaryModels.sort((a, b) => {
    const aPriority = getSummaryModelPriority(a.id);
    const bPriority = getSummaryModelPriority(b.id);

    // ìš°ì„ ìˆœìœ„ê°€ ë‹¤ë¥´ë©´ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
    if (aPriority !== bPriority) {
      return aPriority - bPriority;
    }

    // ìš°ì„ ìˆœìœ„ê°€ ê°™ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¡œ ì •ë ¬ (í° ê²ƒë¶€í„°)
    const aContext = a.context_length || 4000;
    const bContext = b.context_length || 4000;
    if (aContext !== bContext) {
      return bContext - aContext;
    }

    // ë§ˆì§€ë§‰ìœ¼ë¡œ ì•ŒíŒŒë²³ ìˆœ
    return a.id.localeCompare(b.id);
  });

  return summaryModels;
}

// ìš”ì•½ìš© ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
async function getSummaryModels(apiKey: string): Promise<OpenRouterModel[]> {
  // ìºì‹œ í™•ì¸
  if (summaryModelsCache && Date.now() - summaryModelsCache.timestamp < SUMMARY_CACHE_DURATION) {
    // console.log('ğŸ“‹ Using cached summary models');
    return summaryModelsCache.models;
  }

  try {
    // console.log('ğŸ”„ Fetching models for summary...');
    const response = await fetch('https://openrouter.ai/api/v1/models', {
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
    });

    if (!response.ok) {
      throw new Error(`Models API failed: ${response.status}`);
    }

    const data: OpenRouterModelsResponse = await response.json();
    const summaryModels = filterSummaryModels(data.data);

    // ìºì‹œ ì—…ë°ì´íŠ¸
    summaryModelsCache = {
      models: summaryModels,
      timestamp: Date.now()
    };

    // console.log(`âœ… Loaded ${summaryModels.length} summary models`);
    return summaryModels;

  } catch (error) {
    console.error('âŒ Failed to fetch summary models:', error);

    // í´ë°±: í•˜ë“œì½”ë”©ëœ ê¸°ë³¸ ëª¨ë¸ë“¤
    const fallbackModels = [
      { id: 'google/gemini-flash-1.5-8b:free', context_length: 8000 },
      { id: 'google/gemma-2-9b-it:free', context_length: 8000 }
    ] as OpenRouterModel[];

    console.log('âš ï¸ Using fallback summary models');
    return fallbackModels;
  }
}

// í† í° ìºì‹±ì„ ìœ„í•œ Map
const tokenCache = new Map<string, number>();

// í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )
export function estimateTokenCount(messages: ChatMessage[]): number {
  // ë¹ˆ ë©”ì‹œì§€ ì²˜ë¦¬
  if (!messages || messages.length === 0) return 0;

  // ìºì‹œ í‚¤ ìƒì„± (ë©”ì‹œì§€ IDë“¤ í•´ì‹œ)
  const cacheKey = messages.map(m =>
    `${m.role}-${m.content.length}-${m.timestamp || 0}`
  ).join('|');

  // ìºì‹œì—ì„œ ê²°ê³¼ ì°¾ê¸°
  if (tokenCache.has(cacheKey)) {
    return tokenCache.get(cacheKey)!;
  }

  // ìºì‹œ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
  if (tokenCache.size > 1000) {
    // ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© 50ê°œ ì œê±°
    const keys = Array.from(tokenCache.keys()).slice(0, 50);
    keys.forEach(key => tokenCache.delete(key));
  }

  // ë©”ì‹œì§€ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
  const totalText = messages.map(m => m.content).join(' ');

  // ê°„ë‹¨í•œ í† í° ì¶”ì • (í•œêµ­ì–´/ì˜ì–´ í˜¼í•©: 1í† í° â‰ˆ 0.75ë‹¨ì–´)
  const wordCount = totalText.split(/\s+/).length;
  const estimatedTokens = Math.ceil(wordCount / 0.75);

  // ìºì‹œì— ê²°ê³¼ ì €ì¥
  tokenCache.set(cacheKey, estimatedTokens);

  return estimatedTokens;
}

// ìš”ì•½ì´ í•„ìš”í•œì§€ í™•ì¸ (í† í° ê¸°ì¤€)
export function shouldTriggerSummary(
  messages: ChatMessage[],
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): boolean {
  const totalTokens = estimateTokenCount(messages);
  // console.log(`ğŸ“Š Current conversation tokens: ${totalTokens}, threshold: ${config.maxTokensBeforeSummary}`);
  return totalTokens > config.maxTokensBeforeSummary;
}

// ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ë‹¨ìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìœ ì§€í•  ë©”ì‹œì§€ë“¤ ì°¾ê¸°
function findOptimalRetainMessages(
  messages: ChatMessage[],
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): { messagesToSummarize: ChatMessage[], messagesToRetain: ChatMessage[] } {

  if (messages.length === 0) {
    return { messagesToSummarize: [], messagesToRetain: [] };
  }

  let retainTokens = 0;
  let retainIndex = messages.length;

  // console.log(`ğŸ” Finding optimal split for ${messages.length} messages (target: ${config.targetRetainTokens}, max: ${config.maxRetainTokens} tokens)`);

  // ë’¤ì—ì„œë¶€í„° í† í°ì„ ì„¸ë©´ì„œ ì ì ˆí•œ ì§€ì  ì°¾ê¸°
  for (let i = messages.length - 1; i >= 0; i--) {
    const messageTokens = estimateTokenCount([messages[i]]);

    // ëª©í‘œ í† í°ì„ ë„˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ìµœëŒ€í•œ ìœ ì§€
    if (retainTokens + messageTokens <= config.maxRetainTokens) {
      retainTokens += messageTokens;
      retainIndex = i;
      // console.log(`ğŸ“Œ Including message ${i} (${messages[i].role}): +${messageTokens} tokens (total: ${retainTokens})`);
    } else {
      // console.log(`ğŸš« Skipping message ${i} (${messages[i].role}): would exceed max tokens (${retainTokens + messageTokens} > ${config.maxRetainTokens})`);
      break;
    }
  }

  // ìµœì†Œ í† í° ë³´ì¥ ë° ëŒ€í™” ì™„ì „ì„± í™•ë³´
  if (retainTokens < config.minRetainTokens && retainIndex > 0) {
    // console.log(`â¬†ï¸ Expanding retention: ${retainTokens} < ${config.minRetainTokens} (min required)`);
    // ìµœì†Œ í† í°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë” í¬í•¨
    for (let i = retainIndex - 1; i >= 0; i--) {
      const messageTokens = estimateTokenCount([messages[i]]);
      retainTokens += messageTokens;
      retainIndex = i;
      // console.log(`ğŸ“Œ Adding message ${i} (${messages[i].role}) for min tokens: +${messageTokens} tokens (total: ${retainTokens})`);

      if (retainTokens >= config.minRetainTokens) {
        break;
      }
    }
  }

  // console.log(`ğŸ“Š Initial split at index ${retainIndex}: ${messages.length - retainIndex} messages to retain (~${retainTokens} tokens)`);

  // ëŒ€í™” ì™„ì „ì„± í™•ë³´: user-assistant í˜ì–´ê°€ ì™„ì „íˆ ìœ ì§€ë˜ë„ë¡ ì¡°ì •
  const originalIndex = retainIndex;
  retainIndex = ensureConversationCompleteness(messages, retainIndex);

  if (retainIndex !== originalIndex) {
    const newRetainCount = messages.length - retainIndex;
    const newRetainTokens = estimateTokenCount(messages.slice(retainIndex));
    // console.log(`ğŸ”„ Split adjusted for message pairs: ${messages.length - originalIndex} -> ${newRetainCount} messages (~${newRetainTokens} tokens)`);
    retainTokens = newRetainTokens;
  }

  const messagesToSummarize = messages.slice(0, retainIndex);
  const messagesToRetain = messages.slice(retainIndex);

  // console.log(`ğŸ’­ Final split: summarize ${messagesToSummarize.length} messages, retain ${messagesToRetain.length} messages`);
  // console.log(`ğŸ”¢ Token distribution: ~${estimateTokenCount(messagesToSummarize)} to summarize, ~${estimateTokenCount(messagesToRetain)} to retain`);

  // ëŒ€í™” ìŒ ë¬´ê²°ì„± ê²€ì¦
  if (messagesToSummarize.length > 0 && messagesToRetain.length > 0) {
    const lastSummary = messagesToSummarize[messagesToSummarize.length - 1];
    const firstRetain = messagesToRetain[0];
    // console.log(`ğŸ” Split boundary: ${lastSummary.role} (summary) | ${firstRetain.role} (retain)`);

    if (lastSummary.role === 'user' && firstRetain.role === 'assistant') {
      console.warn(`âš ï¸ WARNING: User question separated from assistant answer! This should not happen.`);
    }
  }

  return { messagesToSummarize, messagesToRetain };
}

// ëŒ€í™” ì™„ì „ì„± í™•ë³´ (user-assistant í˜ì–´ ìœ ì§€)
function ensureConversationCompleteness(messages: ChatMessage[], splitIndex: number): number {
  if (splitIndex <= 0 || splitIndex >= messages.length) {
    return splitIndex;
  }

  // user-assistant ìŒì´ ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡ ì¡°ì •
  // ì›ì¹™: user ë©”ì‹œì§€ëŠ” ë°˜ë“œì‹œ ê·¸ì— ëŒ€ì‘í•˜ëŠ” assistant ì‘ë‹µê³¼ í•¨ê»˜ ê°™ì€ ê·¸ë£¹ì— ìˆì–´ì•¼ í•¨

  let adjustedIndex = splitIndex;

  // ë¶„í• ì ì—ì„œ ëŒ€í™” ìŒ í™•ì¸
  for (let i = 0; i < messages.length - 1; i++) {
    const currentMsg = messages[i];
    const nextMsg = messages[i + 1];

    // user -> assistant ìŒì„ ì°¾ìŒ
    if (currentMsg.role === 'user' && nextMsg.role === 'assistant') {
      // userê°€ ìš”ì•½ ë¶€ë¶„ì—, assistantê°€ ìœ ì§€ ë¶€ë¶„ì— ìˆëŠ” ê²½ìš°
      if (i < adjustedIndex && i + 1 >= adjustedIndex) {
        // assistantë¥¼ ìš”ì•½ ë¶€ë¶„ìœ¼ë¡œ ì´ë™ (ìŒì„ ìœ ì§€)
        adjustedIndex = i + 2;
        // console.log(`ğŸ”„ Adjusting split: moved assistant response (${i + 1}) to summary to keep user-assistant pair together`);
      }
      // assistantê°€ ìš”ì•½ ë¶€ë¶„ì—, ë‹¤ìŒ userê°€ ìœ ì§€ ë¶€ë¶„ì— ìˆëŠ” ê²½ìš°
      else if (i + 1 < adjustedIndex && i + 2 < messages.length && i + 2 >= adjustedIndex) {
        // ë‹¤ìŒ ë©”ì‹œì§€ê°€ userì´ë©´ ê·¸ ìŒë„ í™•ì¸í•´ì•¼ í•¨
        continue;
      }
    }
  }

  // ì¡°ì •ëœ ì¸ë±ìŠ¤ê°€ ë©”ì‹œì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í™•ì¸
  if (adjustedIndex > messages.length) {
    adjustedIndex = messages.length;
  }

  // ì¡°ì •ëœ ì§€ì ì—ì„œ ëŒ€í™”ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ëŠì–´ì§€ëŠ”ì§€ ìµœì¢… í™•ì¸
  if (adjustedIndex > 0 && adjustedIndex < messages.length) {
    const lastSummaryMsg = messages[adjustedIndex - 1];
    const firstRetainMsg = messages[adjustedIndex];

    // assistant -> userë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì „í™˜ë˜ëŠ” ê²ƒì´ ì´ìƒì 
    if (lastSummaryMsg.role === 'assistant' && firstRetainMsg.role === 'user') {
      // console.log(`âœ… Clean split: assistant (summary) -> user (retain) at index ${adjustedIndex}`);
    } else if (lastSummaryMsg.role === 'user' && firstRetainMsg.role === 'assistant') {
      // user -> assistant ìŒì´ ë¶„ë¦¬ëœ ê²½ìš°, assistantë¥¼ ìš”ì•½ìœ¼ë¡œ ì´ë™
      adjustedIndex = adjustedIndex + 1;
      // console.log(`ğŸ”„ Final adjustment: moved assistant to summary to complete user-assistant pair`);
    }
  }

  if (adjustedIndex !== splitIndex) {
    // console.log(`ğŸ“ Split index adjusted: ${splitIndex} -> ${adjustedIndex} to preserve message pairs`);
  }

  return adjustedIndex;
}

// ìœ ì§€í•  ìµœê·¼ ë©”ì‹œì§€ ê°œìˆ˜ ê²°ì • (ë™ì ) - ì´ì œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
function getRetainMessageCount(
  totalMessages: number,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): number {
  // í† í° ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ë˜ì–´ ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
  // í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
  return Math.min(totalMessages, 10);
}

// ëŒ€í™” ìš”ì•½ ìƒì„±
export async function summarizeConversation(
  messages: ChatMessage[],
  apiKey: string,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): Promise<string> {

  // ìµœì ì˜ ë¶„í• ì  ì°¾ê¸°
  const { messagesToSummarize, messagesToRetain } = findOptimalRetainMessages(messages, config);

  if (messagesToSummarize.length === 0) {
    return '';
  }

  const conversationText = messagesToSummarize
    .map(m => `${m.role === 'user' ? 'ì‚¬ìš©ì' : 'ì–´ì‹œìŠ¤í„´íŠ¸'}: ${m.content}`)
    .join('\n\n');

  const availableModels = await getSummaryModels(apiKey);

  if (availableModels.length === 0) {
    throw new Error('No summary models available');
  }

  // ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ëª¨ë¸ ì‹œë„ (ìƒìœ„ 2ê°œë§Œ ì‹œë„í•˜ë„ë¡ ì œí•œ)
  for (const model of availableModels.slice(0, 2)) {
    try {
      // íƒ€ì„ì•„ì›ƒ ì„¤ì •
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 10000); // 10ì´ˆ íƒ€ì„ì•„ì›ƒ

      try {
        const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${apiKey}`,
            'Content-Type': 'application/json',
            'HTTP-Referer': 'https://chat.h4o.kim',
            'X-Title': 'Chatty-Summary'
          },
          body: JSON.stringify({
            model: model.id,
            messages: [{
              role: 'user',
              content: SUMMARY_PROMPT.replace('{conversation}', conversationText)
            }],
            max_tokens: config.summaryTargetTokens,
            temperature: 0.3,  // ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ê²Œ
            top_p: 0.9
          }),
          signal: controller.signal
        });

        clearTimeout(timeoutId);

        if (!response.ok) {
          const errorText = await response.text();
          console.warn(`âš ï¸ Summary model ${model.id} failed: ${response.status} - ${errorText}`);
          continue; // ë‹¤ìŒ ëª¨ë¸ ì‹œë„
        }

        const data = await response.json() as any;
        const summary = data.choices?.[0]?.message?.content;

        if (summary) {
          return summary.trim();
        }
      } finally {
        clearTimeout(timeoutId);
      }
    } catch (error) {
      console.warn(`âš ï¸ Summary model ${model.id} error:`, error);
      continue; // ë‹¤ìŒ ëª¨ë¸ ì‹œë„
    }
  }

  throw new Error('All summary models failed');
}

// ìš”ì•½ê³¼ í•¨ê»˜ ë©”ì‹œì§€ êµ¬ì„±
export function buildMessagesWithSummary(
  messages: ChatMessage[],
  existingSummary: string | null,
  newMessage: string,
  systemPrompt: string,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): any[] {

  const result = [];

  // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
  result.push({ role: 'system', content: systemPrompt });

  // ê¸°ì¡´ ìš”ì•½ì´ ìˆìœ¼ë©´ ì¶”ê°€
  if (existingSummary) {
    result.push({
      role: 'system',
      content: `[ì´ì „ ëŒ€í™” ìš”ì•½]\n${existingSummary}\n\n[í˜„ì¬ ëŒ€í™”]`
    });
  }

  // ì „ì²´ ëŒ€í™” + ìƒˆ ë©”ì‹œì§€ì˜ í† í° ìˆ˜ í™•ì¸
  const tempMessages = [...messages, { role: 'user' as const, content: newMessage, timestamp: Date.now() }];
  const totalTokens = estimateTokenCount(tempMessages);

  // í† í° ìˆ˜ê°€ ì„ê³„ê°’ ì´í•˜ë©´ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³´ì¡´ (ë¶„í•  ì•ˆí•¨)
  if (totalTokens <= config.maxTokensBeforeSummary) {
    // ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ í¬í•¨
    messages.forEach(msg => {
      result.push({ role: msg.role, content: msg.content });
    });
  } else {
    // ìµœì ì˜ ë©”ì‹œì§€ ë¶„í•  (í† í° ê¸°ì¤€)
    const { messagesToRetain } = findOptimalRetainMessages(messages, config);

    // ìµœê·¼ ë©”ì‹œì§€ë“¤ë§Œ í¬í•¨
    messagesToRetain.forEach(msg => {
      result.push({ role: msg.role, content: msg.content });
    });
  }

  // ìƒˆ ë©”ì‹œì§€
  result.push({ role: 'user', content: newMessage });

  return result;
}

// ìš”ì•½ ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
export interface SummaryResponse {
  summary: string;
  summarizedMessageCount: number;
  remainingMessages: ChatMessage[];
  totalTokensAfterSummary: number;
}

// ì „ì²´ ìš”ì•½ ì²˜ë¦¬ í•¨ìˆ˜
export async function processSummarization(
  messages: ChatMessage[],
  apiKey: string,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): Promise<SummaryResponse> {

  const summary = await summarizeConversation(messages, apiKey, config);

  // ìµœì ì˜ ë©”ì‹œì§€ ë¶„í• 
  const { messagesToSummarize, messagesToRetain } = findOptimalRetainMessages(messages, config);
  const summarizedCount = messagesToSummarize.length;

  return {
    summary,
    summarizedMessageCount: summarizedCount,
    remainingMessages: messagesToRetain,
    totalTokensAfterSummary: estimateTokenCount(messagesToRetain)
  };
}


