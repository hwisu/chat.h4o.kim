import { ChatMessage, OpenRouterModel, OpenRouterModelsResponse } from '../types';

// ìš”ì•½ ì„¤ì •
export interface SummarizationConfig {
  maxTokensBeforeSummary: number;  // 24000í† í° (128k ì»¨í…ìŠ¤íŠ¸ì˜ ì•½ 20%)
  summaryTargetTokens: number;     // 800í† í° (ì§§ê²Œ)
  targetRetainTokens: number;      // 10000í† í° (ìµœê·¼ ëŒ€í™” ìœ ì§€ ëª©í‘œ)
  minRetainTokens: number;         // 6000í† í° (ìµœì†Œ ìœ ì§€)
  maxRetainTokens: number;         // 15000í† í° (ìµœëŒ€ ìœ ì§€)
}

export const DEFAULT_SUMMARY_CONFIG: SummarizationConfig = {
  maxTokensBeforeSummary: 24000,
  summaryTargetTokens: 800,
  targetRetainTokens: 10000,
  minRetainTokens: 6000,
  maxRetainTokens: 15000
};

// Summary prompt
const SUMMARY_PROMPT = `
Summarize the following conversation between a user and an AI assistant.
Focus on key points, questions, and information shared.
Be comprehensive yet concise, preserving all important context for continuing the conversation.
Your summary will be used as context for continuing the conversation, so make sure to include any relevant details.
`;

// ìš”ì•½ìš© ëª¨ë¸ ìºì‹œ (5ë¶„)
let summaryModelsCache: { models: OpenRouterModel[], timestamp: number } | null = null;
const SUMMARY_CACHE_DURATION = 5 * 60 * 1000; // 5ë¶„

// ìš”ì•½ìš© ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°
function getSummaryModelPriority(modelId: string): number {
  if (modelId.includes('google') && modelId.includes('flash') && modelId.includes('free')) {
    return 1;
  }

  if (modelId.includes('gemma-3') && modelId.includes('free')) {
    return 2;
  }

  if (modelId.includes('google') && modelId.includes('free')) {
    return 3;
  }

  if (modelId.includes('free')) {
    return 4;
  }

  return 5;
}

// ìš”ì•½ìš© ëª¨ë¸ í•„í„°ë§ ë° ì •ë ¬
function filterSummaryModels(models: OpenRouterModel[]): OpenRouterModel[] {
  const freeModels = models.filter(model => model.id.includes(':free'));

  return freeModels.sort((a, b) => {
    const priorityA = getSummaryModelPriority(a.id);
    const priorityB = getSummaryModelPriority(b.id);

    if (priorityA !== priorityB) {
      return priorityA - priorityB;
    }

    const contextLengthA = a.context_length || 0;
    const contextLengthB = b.context_length || 0;

    if (contextLengthA !== contextLengthB) {
      return contextLengthB - contextLengthA;
    }

    return a.id.localeCompare(b.id);
  });
}

// ìš”ì•½ìš© ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì™„ì „íˆ ë™ì )
async function getSummaryModels(apiKey: string): Promise<OpenRouterModel[]> {
  if (summaryModelsCache &&
      Date.now() - summaryModelsCache.timestamp < SUMMARY_CACHE_DURATION) {
    return summaryModelsCache.models;
  }

  try {
    const response = await fetch('https://openrouter.ai/api/v1/models', {
      method: 'GET',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://chat.h4o.kim',
        'X-Title': 'Chatty'
      }
    });

    if (response.ok) {
      const data = await response.json() as OpenRouterModelsResponse;
      const summaryModels = filterSummaryModels(data.data);

      summaryModelsCache = {
        models: summaryModels,
        timestamp: Date.now()
      };

      console.log(`ğŸ“Š Summary models available (first 3):`, 
        summaryModels.slice(0, 3).map(m => `${m.id}: ${m.context_length} tokens`).join('\n  ')
      );

      return summaryModels;
    } else {
      console.warn(`Failed to fetch models: ${response.status} ${response.statusText}`);
      throw new Error(`Failed to fetch models: ${response.status}`);
    }
  } catch (error) {
    console.warn('Failed to fetch summary models from API:', error);
    throw new Error(`Could not fetch available models: ${error}`);
  }
}

// í•˜ë“œì½”ë”©ëœ í´ë°± ëª¨ë¸ ì œê±° - í•­ìƒ ë™ì ìœ¼ë¡œ ë°›ì•„ì˜¨ ëª¨ë¸ë§Œ ì‚¬ìš©

// í† í° ìºì‹±ì„ ìœ„í•œ Map
const tokenCountCache = new Map<string, number>();

// í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )
export function estimateTokenCount(messages: ChatMessage[]): number {
  if (!messages || messages.length === 0) return 0;

  const cacheKey = messages.map(m => m.role + m.content.substring(0, 10)).join('|');

  if (tokenCountCache.has(cacheKey)) {
    return tokenCountCache.get(cacheKey)!;
  }

  if (tokenCountCache.size > 1000) {
    const keysToDelete = [...tokenCountCache.keys()].slice(0, 50);
    keysToDelete.forEach(key => tokenCountCache.delete(key));
  }

  const allText = messages.map(msg => msg.content).join(' ');

  const wordCount = allText.split(/\s+/).length;
  const estimatedTokens = Math.ceil(wordCount / 0.75);

  tokenCountCache.set(cacheKey, estimatedTokens);

  return estimatedTokens;
}

// ìš”ì•½ì´ í•„ìš”í•œì§€ í™•ì¸ (í† í° ê¸°ì¤€ + ë©”ì‹œì§€ ìˆ˜ ê¸°ì¤€)
export function shouldTriggerSummary(
  messages: ChatMessage[],
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): boolean {
  // ìµœì†Œ ë©”ì‹œì§€ ìˆ˜ ì²´í¬ (ìš”ì•½í•˜ê¸°ì— ì¶©ë¶„í•œ ëŒ€í™”ê°€ ìˆì–´ì•¼ í•¨)
  if (messages.length < 10) {
    return false;
  }
  
  const estimatedTokens = estimateTokenCount(messages);
  
  // í† í° ìˆ˜ê°€ ê¸°ì¤€ì„ ë„˜ì–´ì•¼ í•˜ê³ , ìµœì†Œí•œì˜ ëŒ€í™” ì„¸ì…˜ì´ ìˆì–´ì•¼ í•¨
  return estimatedTokens > config.maxTokensBeforeSummary;
}

// ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ë‹¨ìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìœ ì§€í•  ë©”ì‹œì§€ë“¤ ì°¾ê¸°
function findOptimalRetainMessages(
  messages: ChatMessage[],
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): { messagesToSummarize: ChatMessage[], messagesToRetain: ChatMessage[] } {
  if (messages.length < 4) {
    return {
      messagesToSummarize: [],
      messagesToRetain: messages
    };
  }

  let splitIndex = messages.length;

  while (splitIndex > 2) {
    const messagesToRetain = messages.slice(splitIndex - 1);
    const retainTokenEstimate = estimateTokenCount(messagesToRetain);

    if (retainTokenEstimate >= config.minRetainTokens &&
        retainTokenEstimate <= config.maxRetainTokens) {
      break;
    }

    if (retainTokenEstimate > config.maxRetainTokens) {
      splitIndex += 2;
      break;
    }

    splitIndex -= 2;
  }

  splitIndex = Math.max(4, ensureConversationCompleteness(messages, splitIndex));
  splitIndex = Math.min(messages.length - 2, splitIndex);

  return {
    messagesToSummarize: messages.slice(0, splitIndex),
    messagesToRetain: messages.slice(splitIndex)
  };
}

// ëŒ€í™” ì™„ì „ì„± í™•ë³´ (user-assistant í˜ì–´ ìœ ì§€)
function ensureConversationCompleteness(messages: ChatMessage[], splitIndex: number): number {
  if (splitIndex <= 0 || splitIndex >= messages.length) {
    return splitIndex;
  }

  const nextMessage = messages[splitIndex];
  const previousMessage = messages[splitIndex - 1];

  if (
    (nextMessage.role === 'assistant' && previousMessage.role === 'user') ||
    (nextMessage.role === 'user' && previousMessage.role === 'assistant')
  ) {
    return splitIndex;
  }

  if (nextMessage.role === 'assistant') {
    if (splitIndex + 1 < messages.length) {
      return splitIndex + 1;
    }
    return splitIndex;
  }

  if (nextMessage.role === 'user') {
    if (splitIndex - 1 > 0) {
      return splitIndex - 1;
    }
    return splitIndex;
  }

  return splitIndex;
}

// ëŒ€í™” ìš”ì•½ ìƒì„± (ì™„ì „íˆ ë™ì  ëª¨ë¸ ì„ íƒ)
export async function summarizeConversation(
  messages: ChatMessage[],
  apiKey: string,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG,
  fallbackModelId?: string // ì‚¬ìš©ìê°€ í˜„ì¬ ì„ íƒí•œ ëª¨ë¸ì„ í´ë°±ìœ¼ë¡œ ì‚¬ìš©
): Promise<string> {
  if (messages.length < 4) {
    return '';
  }

  let summaryModels: OpenRouterModel[] = [];
  
  try {
    summaryModels = await getSummaryModels(apiKey);
  } catch (error) {
    console.warn('Could not fetch summary models dynamically:', error);
    
    // ë™ì  ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‚¬ìš©ì ëª¨ë¸ì„ í´ë°±ìœ¼ë¡œ ì‚¬ìš©
    if (fallbackModelId) {
      console.log(`ğŸ”„ Using current user model as summary fallback: ${fallbackModelId}`);
      summaryModels = [{
        id: fallbackModelId,
        name: fallbackModelId.split('/').pop() || fallbackModelId,
        context_length: 131072,
        architecture: {
          input_modalities: ['text'],
          output_modalities: ['text'],
          tokenizer: 'unknown'
        },
        pricing: {
          prompt: '0',
          completion: '0'
        }
      }];
    } else {
      throw new Error('No summary models available and no fallback model provided');
    }
  }

  if (!summaryModels || summaryModels.length === 0) {
    throw new Error('No summary models available');
  }

  const conversationText = messages.map(msg => {
    const role = msg.role.toUpperCase();
    return `${role}: ${msg.content}`;
  }).join('\n\n');

  // ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
  for (let i = 0; i < Math.min(3, summaryModels.length); i++) {
    const summaryModelId = summaryModels[i].id;
    
    try {
      console.log(`ğŸ”„ Attempting summarization with model: ${summaryModelId}`);
      
      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
          'HTTP-Referer': 'https://chat.h4o.kim',
          'X-Title': 'Chatty Context Summarization'
        },
        body: JSON.stringify({
          model: summaryModelId,
          messages: [
            {
              role: 'system',
              content: SUMMARY_PROMPT
            },
            {
              role: 'user',
              content: conversationText
            }
          ],
          max_tokens: config.summaryTargetTokens,
          temperature: 0.3,
          frequency_penalty: 0.5,
          presence_penalty: 0.5
        })
      });

      if (response.ok) {
        const result: any = await response.json();

        if (result.choices && result.choices.length > 0) {
          const summary = result.choices[0].message.content.trim();
          console.log(`âœ… Summarization successful with model: ${summaryModelId}`);
          return summary;
        }
      } else {
        console.warn(`âŒ Summarization failed with model ${summaryModelId}: ${response.status} ${response.statusText}`);
        
        if (i === summaryModels.length - 1) {
          throw new Error(`All summary models failed. Last error: ${response.status} ${response.statusText}`);
        }
        // ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
      }
    } catch (error) {
      console.warn(`âŒ Error with summary model ${summaryModelId}:`, error);
      
      if (i === Math.min(3, summaryModels.length) - 1) {
        throw new Error(`All summary models failed. Last error: ${error}`);
      }
      // ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
    }
  }

  throw new Error('Failed to generate summary with any available model');
}

// ìš”ì•½ê³¼ í•¨ê»˜ ë©”ì‹œì§€ êµ¬ì„±
export function buildMessagesWithSummary(
  messages: ChatMessage[],
  existingSummary: string | null,
  newMessage: string,
  systemPrompt: string,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG
): any[] {
  const apiMessages = [];

  apiMessages.push({
    role: 'system',
    content: systemPrompt
  });

  if (existingSummary) {
    apiMessages.push({
      role: 'system',
      content: `Previous conversation summary: ${existingSummary}`
    });
  }

  for (const message of messages) {
    apiMessages.push({
      role: message.role,
      content: message.content
    });
  }

  if (newMessage) {
    apiMessages.push({
      role: 'user',
      content: newMessage
    });
  }

  return apiMessages;
}

// ìš”ì•½ ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
export interface SummaryResponse {
  summary: string;
  summarizedMessageCount: number;
  remainingMessages: ChatMessage[];
  totalTokensAfterSummary: number;
}

// ì „ì²´ ìš”ì•½ ì²˜ë¦¬ í•¨ìˆ˜
export async function processSummarization(
  messages: ChatMessage[],
  apiKey: string,
  config: SummarizationConfig = DEFAULT_SUMMARY_CONFIG,
  currentUserModel?: string // í˜„ì¬ ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸
): Promise<SummaryResponse> {
  if (!shouldTriggerSummary(messages, config)) {
    return {
      summary: '',
      summarizedMessageCount: 0,
      remainingMessages: messages,
      totalTokensAfterSummary: estimateTokenCount(messages)
    };
  }

  const { messagesToSummarize, messagesToRetain } = findOptimalRetainMessages(messages, config);

  if (messagesToSummarize.length < 4) {
    return {
      summary: '',
      summarizedMessageCount: 0,
      remainingMessages: messages,
      totalTokensAfterSummary: estimateTokenCount(messages)
    };
  }

  try {
    const summary = await summarizeConversation(messagesToSummarize, apiKey, config, currentUserModel);

    return {
      summary,
      summarizedMessageCount: messagesToSummarize.length,
      remainingMessages: messagesToRetain,
      totalTokensAfterSummary:
        estimateTokenCount(messagesToRetain) +
        Math.ceil(summary.split(/\s+/).length / 0.75)
    };
  } catch (error) {
    throw new Error(`Summary generation failed: ${error}`);
  }
}


